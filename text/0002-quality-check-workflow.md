# Table of Contents <!-- omit in toc -->

- [Introduction](#introduction)
- [Motivation](#motivation)
- [Discussion](#discussion)
- [Specification](#specification)
- [Items Still In-Progress](#items-still-in-progress)
- [Outstanding Questions](#outstanding-questions)

## Introduction

This RFC documents an automated workflow for assessing the integrity and quality of St. Jude Cloud genomics data. The goal of this RFC is two-fold.

1. Establish state of the art method for comprehensively evaluating genomics data quality at scale, both at time of receipt and after processing.
2. Publish a collection of metrics that end-users of St. Jude Cloud can leverage to assess the quality of the data available. This context should save users time computing the information themselves while also informing appropriate use of the data.

You can find the relevant discussion on the [associated pull request](https://github.com/stjudecloud/rfcs/pull/3).

## Motivation

St. Jude Cloud is a large repository of omics data available for request from
the academic and non-profit community. As such, the project processes thousands
of samples from whole genome, whole exome, RNA-Seq, and various omics-based
assays each year. A standard, robust method to assess pre-processing and
post-processing quality for samples has been developed in-house, but there are
some shortcomings with our current approach. In particular, this RFC will:

- Define the standard set of QC tools used to evaluating omics-based data,
- identify and implement key metrics that can be automated to assist in manual observation of the data, and
- outline mechanisms to publish those QC results alongside the data already in
  St. Jude Cloud so that end-users can leverage this information.

## Discussion

### Types of QC

There are (at least) two different types of QC typically carried out omics-based data. **Experimental** QC attempts to identify the success of the assay(s) performed. Once one is sufficiently satisfied that the data generated by an experiment is "good", **computational** QC examines the degree to which computational processing of that data was completed successfully.

By the time data reaches the St. Jude Cloud team from various sources, extensive
_experimental_ and _computational_ evaluation have already been carried out.
Each contributing project has its own thresholds for quality in both areas,
which is dependent on the best practices at that point in time and the goals of
the project. Most often, our team takes the computational data, reverts it back
to its raw form (such as FastQ files), and reprocesses the data using a
harmonization pipeline.

Thus, the scope of this RFC, and the QC of samples on the project in general, is
limited to the _computational_ QC of the files produced for publication in St.
Jude Cloud. While we do produce results that define _experimental_ results (such
as `fastqc`), these are rarely used to decide which files pass or fail our
QC—this is in recognition of the fact that the data, while not always perfect,
is extremely valuable due to its relative scarcity. We hope that the inclusion
of these results will save end-users time and aid in decision-making about
downstream analysis approaches.

| Name                                                                                                       | Tool        | Description                                                                                                                                                                                              | WGS                                            | WXS                                            | RNA-Seq                                        | ChIP-Seq                                       |
| ---------------------------------------------------------------------------------------------------------- | ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- |
| M Reads Mapped ([link](#m-reads-mapped))                                                                   | [samtools]  | Number of reads mapped in millions. This metric is useful                                                                                                                                                | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) |
| Percentage of Reads Aligned ([link](#percentage-of-reads-aligned))                                         | [picard]    | Number of mapped reads divided by the total number of reads as a percentage.                                                                                                                             | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![No](https://img.shields.io/badge/no-red)     |
| Median Insert Size ([link](#median-insert-size))                                                           | [picard]    | Median size of the fragment that is inserted between the sequencing adapters (estimated in silico).                                                                                                      | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![No](https://img.shields.io/badge/no-red)     | ![No](https://img.shields.io/badge/no-red)     |
| Percentage Duplication ([link](#percentage-duplication))                                                   | [picard]    | Percentage of the reads that are marked as PCR or optical duplicates.                                                                                                                                    | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![No](https://img.shields.io/badge/no-red)     | ![Yes](https://img.shields.io/badge/yes-green) |
| ≥ 30X Coverage ([link](#-30x-coverage))                                                                    | [mosdepth]  | The percentage of locations that are covered by at least 30 reads for the whole genome, the exonic regions, and the coding sequence regions specifically.                                                | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![No](https://img.shields.io/badge/no-red)     | ![No](https://img.shields.io/badge/no-red)     |
| Predicted Instrument ([link](#predicted-instrument))                                                       | [ngsderive] | The predicted sequencing machine that produced this data. This should match your expectations for what machine sequenced the data.                                                                       | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) |
| Predicted Read Length ([link](#predicted-read-length))                                                     | [ngsderive] | The predicted read length for the sequencing experiment. This should match your expectations for how the library was prepared. A value of -1 indicates that a stable read length could not be predicted. | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) |
| Probable Encoding ([link](#probable-encoding))                                                             | [ngsderive] | The predicted encoding of the _original_ FASTQ file. For this, all modern data should match `Sanger/Illumina 1.8`.                                                                                       | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) |
| Percentage of Reads attributed to Homo sapiens ([link](#percentage-of-reads-attributed-to-homo-sapiens))   | [kraken]    | The percentage of reads that were assigned as originating from Homo sapiens from [kraken].                                                                                                               | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) |
| Percentage of Reads attributed to top 5 species ([link](#percentage-of-reads-attributed-to-top-5-species)) | [kraken]    | The percentage of reads that were assigned to the top 5 reported species from [kraken].                                                                                                                  | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) |
| Percentage of Reads Unclassified ([link](#percentage-of-reads-unclassified))                               | [kraken]    | The percentage of reads that were not classified by [kraken].                                                                                                                                            | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) | ![Yes](https://img.shields.io/badge/yes-green) |



## General statistics

### **M Reads Mapped** 

_Produced by [samtools]_

Number of reads mapped in millions. The number of reads in a whole genome
sequencing file can vary widely based on the experiment design and the
sequencing depth of the project.
  * Typically, WGS files contain between 200M reads up to 2 billion reads (or
    higher in some cases). Any deviation outside of this range should be
    investigated further (particularly if the number is lower, which suggests
    that the file may be truncated).
  * This number is especially informative within the context of samples from the
    same cohort. We expect a normal distribution for this metric within the same
    sequencing cohort, special attention is required for any samples that do not
    conform to this distribution. 

### **Percentage of Reads Aligned** 

_Produced by [picard]_

Percentage of the number of reads that were able to be mapped to a given
  reference genome. Within St. Jude Cloud, this metric is indicative of the
  amount of non-human material which was sequenced during the experiment.
  * This number is especially informative within the context of samples from the
    same cohort. We expect the percentage of aligned reads to be tightly
    clustered above 95% mapping rate. It is not uncommon for samples to go as
    low as 80% mapping rate, and this should be considered within an acceptable
    range. Occasionally you may find small proportion of samples between 51%-79%
    mapping rate: those samples should be investigated, but typically, you
    should still include those samples in the release. Anything below 50%
    mapping rate signals an issue with contamination, and anything less than 30%
    mapping may be an issue with the mapper or the data integrity (e.g.
    mismatched read pairs).

### **Median Insert Size** 

_Produced by [picard]_

Size of the fragment inserted between the sequencing adapters. In whole genome
  sequencing experiments, typically you have a target fragment length for each
  sample (commonly ~2x the read length to maximize the amount of information
  gathered per fragment). The median insert size has proven to be an incredibly
  valuable statistic for identifying mapping problems and experimental
  abnormalities.
  * Typically, the median insert size should be between 1.5x - 2x the read
    length of the experiment. This criteria may be evaluated loosely, as insert
    sizes that are close to this range (anything between 1x - 5x read length) do
    not necessarily indicate a major problem. Samples with an extremely low
    median insert size (10-40 nucleotides) are likely the cause of mapping
    artifacts—particularly if `bwa mem` was used with a low seed size.
  * Insert size distributions tend to follow a [gamma
    distribution](https://en.wikipedia.org/wiki/Gamma_distribution) where long,
    rolling peaks are interspersed with tight peaks close to the target insert
    size.
  * This number is especially informative within the context of samples from the
    same cohort. We expect the median insert size across samples to be tightly
    clustered within a cohort.

### **≥ 30X Coverage** 

_Produced by [mosdepth]_

Percentage of the genome which has at least 30 reads covering each position (on
  average) for the whole genome, the exonic regions, and the coding regions
  ("30x coverage"). 30x coverage at any location is widely considered to be the
  minimum number of reads needed to generate high confidence genotype call. In
  whole genome sequencing, it is desirable to have as much of the genome as
  possible to be covered by 30 or more reads. Expected genome coverage is often
  determined at the time of sequencing and may vary from project to project
  depending on cost and experimental design.
  * Target sequencing depth and fraction of the genome to be covered are
    generally set on a project by project basis. Speak with the someone on the
    sequencing team to ensure you understand what sequencing depth and fraction
    of the genome covered you should be expecting.
  * At a minimum, we expect 80% of the whole genome to be covered by at least 30
    reads in high-quality whole genome sequencing experiments. For higher depth
    coverage projects like our Clinical Genomics project, the standard is 60x
    across 80% of the genome.
  * Less coverage does not necessarily indicate a problem, but in whole genome
    sequencing, anything below 65% of the genome at 30X signals that something
    may be going wrong. Pay close attention to the distribution of the reads
    across the genome by generating a coverage plot to see where sequencing bias
    is being introduced.
  * This number is especially informative within the context of samples from the
    same cohort. We expect this metric to be tightly clustered across samples
    within a cohort.

### **GC Content** 

_Produced by [FastQC]_

Percentage of the nucleotides contained within reads which are Cs (Cytosine) or
Gs (Guanine). GC content is important because Gs pair with Cs in DNA with 3
hydrogen bonds whereas As pair with Ts in DNA with 2 hydrogen bonds. Ergo, DNA
is considered to be stronger when comprised of more GC bonds, and GC content is
a defining characteristic of different genomes. GC content for humans is
[estimated to be at around 41%](https://www.nature.com/articles/35057062#Sec15).
  * GC content in whole genome sequencing is typically between 40%-60% depending
    on sequencing bias. Any deviation outside of this range should be
    investigated further.
  * This number is especially informative within the context of samples from the
    same cohort. We expect GC content across samples to be tightly clustered
    within a cohort.
  * If either of the above assumptions do not hold true, probable sources of
    variance include library preparation abnormalities or contamination issues.

## Specification

These are generic instructions for running each of the tools in our pipeline. We run our pipeline as a [WDL workflow](https://github.com/stjudecloud/workflows/blob/master/workflows/qc/quality-check-standard.wdl). We have supplied examples of the commands used for each package. For the typical memory requirements of each command, please see our [WDL repository](https://github.com/stjudecloud/workflows).

### Dependencies

We presume Anaconda is available and installed. If not, please follow the link to [Anaconda](https://www.anaconda.com/) first.

```bash
conda create --name bio-qc \
    --channel bioconda \
    --channel conda-forge \
    fastqc==0.11.8 \
    picard==2.20.2  \
    qualimap==2.2.2c \
    samtools==1.9 \
    fastq-screen==0.13.0 \
    ngsderive==2.2.0 \
    multiqc==1.10.1 \
    -y

conda activate bio-qc
```

For linting created fastqs, `fqlib` must be installed. See installation instructions [here](https://github.com/stjude/fqlib).

### Workflow

The workflow specification is as follows. Note that arguments that are not integral to the command (such as output directories) or vary between compute environments (such as memory thresholds or number of threads) are not included.

1. Compute the md5 checksum of the file.

      ```bash
      md5sum $BAM
      ```

2. Use Picard's `ValidateSamFile` tool to ensure the inner contents of the BAM file are well-formed.

      ```bash
      picard ValidateSamFile \
          I=$BAM             \  # specify bam file
          MODE=SUMMARY          # concise output
      ```

3. Run `samtools flagstat` to gather general statistics such as alignment percentage.

      ```bash
      samtools flagstat $BAM
      ```

4. Run `fastqc` to collect sequencing and library-related statistics. These are only for informational purposes -- as stated above, we typically do not remove samples based on this information (with rare exception), as the sequencing-related QC work was done upstream in the genomics lab.

      ```bash
      fastqc $BAM
      ```

5. Run `ngsderive instrument` to infer sequencing instrument.

      ```bash
      ngsderive instrument $BAM
      ```

6. Run `ngsderive readlen` to infer read lengths.

      ```bash
      ngsderive readlen $BAM
      ```

7. Run `ngsderive encoding` to infer PHRED score encoding.

      ```bash
      ngsderive encoding \
          -n -1          \  # parse the entire file
          $BAM
      ```

8. Run `qualimap bamqc` to gather more in-depth statistics about read stats, coverage, mapping quality, mismatches, etc.

      ```bash
      qualimap bamqc -bam $BAM \  # bam filename
          -nt $NUM_THREADS     \  # threads requested
          -nw 400                 # number of windows
      ```

9. If WGS or WES data, run `fastq_screen`. For performance, we subsample the input BAM using `samtools view -s $computed_fraction` before running it through `picard SamToFastq`. The resulting fastqs are validated with `fq lint` provided by `fqlib`.

      ```bash
      cat $fastq_1 $fastq_2 > $combined_fastq
      fastq_screen          \
          --aligner bowtie2 \
          $combined_fastq
      ```

10. If RNA-Seq data, run `ngsderive strandedness` to determine a backwards-computed strandedness of the RNA-Seq experiment.

      ```bash
      ngsderive strandedness $BAM
      ```

11. If RNA-Seq data, run `ngsderive junction-annotation` to calculate the number of known, novel, and partial-novel junctions.

      ```bash
      ngsderive junction-annotation $BAM
      ```

12. If RNA-Seq data, run `qualimap rnaseq` to gather QC statistics that are tailored for RNA-Seq files.

      ```bash
      qualimap rnaseq --java-mem-size=$MEM_SIZE \  # memory
         -bam $BAM                              \  # bam filename
         [-pe]                                     # specify paired end if paired end
      ```

13. Combine all of the above metrics using `multiqc`.

      ```bash
      multiqc . # recurse all files in '.'
      ```

## Items Still In-Progress

- [ ] Analysis tools for other types of sequencing (ChIP-seq)
- [ ] Useful metadata from various stages (sample collection, laboratory, pre-sequencing, sequencing, post-sequencing)

## Outstanding Questions

- What thresholds or metrics differentiate a poor-quality sample from a high-quality one?
- What other metrics or properties would be valuable?
- What is the best way to define and handle outliers?
- What is the best way to examine cohort integrity? This means experimental category-based tests of samples to find outliers that are of sufficient quality if examined alone. Outliers in this case may indicate classification errors or rare biological conditions. Which metrics are best tested here?

[FastQC]: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/
[multiqc]: https://multiqc.info/
[picard]: https://broadinstitute.github.io/picard/
[mosdepth]: https://github.com/brentp/mosdepth
[qualimap]: http://qualimap.bioinfo.cipf.es/doc_html/command_line.html
[samtools]: http://www.htslib.org/doc/samtools.html
[ngsderive]: https://github.com/stjudecloud/ngsderive/
[kraken]: https://github.com/DerrickWood/kraken2
