{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genome Comparison\n",
    "\n",
    "This notebook contains the code used to compare whether different popular project's `GRCh38` reference FASTA was derived from the `GRCh38_no_alt` set. It contains the links that were used to track down the reference genome for each project, the code to download and prepare the reference genome as specified in each project's docs, and the final generation of a sequence dictionary for each FASTA to do the comparison. \n",
    "\n",
    "In particular, the goal is to determine whether the autosomes, the sex chromosomes, and the decoy sequences that all genomes share (for instance, the EBV sequence) have the same MD5 sum as the original no alt set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's ensure you have all the command line tools needed. For this notebook, I have preinstalled [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) and I'm using anaconda + bioconda to install any dependencies not already included with Ubuntu 18.04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Uncomment the lines below to create the conda environment.\n",
    "# - I'm not necessarily advocating you do this in this notebook, \n",
    "#   probably want to set it up in a terminal and then start this Jupyter notebook after :).\n",
    "# - `gunzip` is assumed to be available on any machine by default.\n",
    "\n",
    "# conda create -n genome-comparison \\\n",
    "#              -c anaconda \\\n",
    "#              -c bioconda \\\n",
    "#              wget \\\n",
    "#              picard \\\n",
    "#              -y\n",
    "# conda init bash\n",
    "# source ~/.bashrc\n",
    "# conda activate genome-comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd just like to check that you have everything on the `$PATH`, you can run this snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "FAIL=false\n",
    "commands_needed=(wget gunzip picard)\n",
    "for CMD in ${commands_needed[*]}; do\n",
    "  if ! which $CMD &>/dev/null; then\n",
    "    >&2 echo \"- \\`$CMD\\` must be available on the \\$PATH!\"\n",
    "    FAIL=true\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [[ $FAIL = true ]]; then \n",
    "  >&2 echo \"\"\n",
    "  >&2 echo \"Please add the above command line tools to your \\$PATH before continuing! Note that some of tools (\\`picard\\` specifically) should be installed with bioconda (see instructions above). If they are not, you may need to edit these commands manually.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original no alt analysis set\n",
    "\n",
    "It's always good to have a baseline, right? For our baseline, we will be using the officially hosted `GRCh38_no_alt` analysis set FASTA from the NCBI (and also recommended by [Heng Li on his blog](https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use)). That's a great read by the way, you should check it out if you have a few minutes (like, say, as you are running this)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== NCBI ==\n",
      "  [*] Downloading NCBI reference genome...\n",
      "  [*] Unzipping NCBI reference genome...\n",
      "  [*] Creating the NCBI sequence dictionary...\n",
      "INFO\t2019-07-07 00:02:07\tCreateSequenceDictionary\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    CreateSequenceDictionary -R NCBI.fa -O NCBI.fa.dict\n",
      "**********\n",
      "\n",
      "\n",
      "00:02:08.148 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/claymcleod/conda/envs/star-mapping/share/picard-2.20.2-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Sun Jul 07 00:02:08 CDT 2019] CreateSequenceDictionary OUTPUT=NCBI.fa.dict REFERENCE=NCBI.fa    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Sun Jul 07 00:02:08 CDT 2019] Executing as claymcleod@desktop-ubuntu on Linux 4.18.0-25-generic amd64; OpenJDK 64-Bit Server VM 11.0.1+13-LTS; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.2-SNAPSHOT\n",
      "[Sun Jul 07 00:02:23 CDT 2019] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.26 minutes.\n",
      "Runtime.totalMemory()=1048576000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "FASTA_SOURCE=\"NCBI\"\n",
    "FASTA_URL=\"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz\"\n",
    "\n",
    ">&2 echo \"== $FASTA_SOURCE ==\"\n",
    ">&2 echo \"  [*] Downloading $FASTA_SOURCE reference genome...\"\n",
    "wget $FASTA_URL -O \"$FASTA_SOURCE\".fa.gz -q --continue\n",
    "\n",
    ">&2 echo \"  [*] Unzipping $FASTA_SOURCE reference genome...\"\n",
    "gunzip -c \"$FASTA_SOURCE\".fa.gz > \"$FASTA_SOURCE\".fa\n",
    "\n",
    ">&2 echo \"  [*] Creating the $FASTA_SOURCE sequence dictionary...\"\n",
    "picard CreateSequenceDictionary R=\"$FASTA_SOURCE\".fa O=\"$FASTA_SOURCE\".fa.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODE Project\n",
    "\n",
    "The ENCODE project stores a reference to all of it's currently used reference files [here](https://www.encodeproject.org/data-standards/reference-sequences/). From that page, you can see that the base reference genome can be downloaded [here](https://www.encodeproject.org/files/GRCh38_no_alt_analysis_set_GCA_000001405.15/@@download/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta.gz). However, we'd like to do a complete analysis including of all of the sequences they use for decoys/viruse/etc. if we want to use them later. Here's the steps I took to find the complete set of reference FASTAs they used in their STAR index:\n",
    "\n",
    "1. Searching for their RNA-seq pipeline yields their specification pretty quickly ([here](https://www.encodeproject.org/pages/pipelines/#RNA-seq)).\n",
    "2. The pipeline we are looking for is [this one](https://www.encodeproject.org/pipelines/ENCPL002LPE/).\n",
    "3. At the bottom of the page, you will see a PDF that is a comprehensive overview of their pipelines and contains a list to all of the current ENCODE reference accessions ([link](https://www.encodeproject.org/documents/6354169f-86f6-4b59-8322-141005ea44eb/@@download/attachment/Long%20RNA-seq%20pipeline%20overview.pdf)).\n",
    "4. In that document, you find that the link to their most currently built `STAR` genome is [here](https://www.encodeproject.org/references/ENCSR314WMD/). \n",
    "5. That page gives you every `STAR` index they use at ENCODE. Selectthe `GRCh38`-based one ([here](https://www.encodeproject.org/files/ENCFF742NER/)).\n",
    "6. Finally, you can see of all of the files from which this `STAR` index was derived from. I think that's a pretty nice feature for data providence! \n",
    "\n",
    "For our purposes, we can just concatenate them for our purposes. Note that the names I've given them in this list are derived from the metadata tags on each of those pages.\n",
    "  * [Spikes.fixed.fasta.gz](https://www.encodeproject.org/files/ENCFF001RTP/@@download/ENCFF001RTP.fasta.gz)\n",
    "  * [PhiX.fasta.gz](https://www.encodeproject.org/files/ENCFF335FFV/@@download/ENCFF335FFV.fasta.gz)\n",
    "  * [GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta.gz](https://www.encodeproject.org/files/GRCh38_no_alt_analysis_set_GCA_000001405.15/@@download/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== ENCODE ==\n",
      "  [*] Downloading ENCODE reference genome...\n",
      "  [*] Downloading SpikeIn reference FASTA...\n",
      "  [*] Downloading PhiX reference FASTA...\n",
      "  [*] Unzipping fastas...\n",
      "  [*] Creating the ENCODE sequence dictionary...\n",
      "INFO\t2019-07-07 00:04:05\tCreateSequenceDictionary\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    CreateSequenceDictionary -R ENCODE.fa -O ENCODE.fa.dict\n",
      "**********\n",
      "\n",
      "\n",
      "00:04:06.015 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/claymcleod/conda/envs/star-mapping/share/picard-2.20.2-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Sun Jul 07 00:04:06 CDT 2019] CreateSequenceDictionary OUTPUT=ENCODE.fa.dict REFERENCE=ENCODE.fa    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Sun Jul 07 00:04:06 CDT 2019] Executing as claymcleod@desktop-ubuntu on Linux 4.18.0-25-generic amd64; OpenJDK 64-Bit Server VM 11.0.1+13-LTS; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.2-SNAPSHOT\n",
      "[Sun Jul 07 00:04:21 CDT 2019] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.27 minutes.\n",
      "Runtime.totalMemory()=1048576000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "FASTA_SOURCE=\"ENCODE\"\n",
    "FASTA_URL=\"https://www.encodeproject.org/files/GRCh38_no_alt_analysis_set_GCA_000001405.15/@@download/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta.gz\"\n",
    "PHIX_FASTA=\"https://www.encodeproject.org/files/ENCFF335FFV/@@download/ENCFF335FFV.fasta.gz\"\n",
    "SPIKEIN_FASTA=\"https://www.encodeproject.org/files/ENCFF001RTP/@@download/ENCFF001RTP.fasta.gz\"\n",
    "\n",
    ">&2 echo \"== $FASTA_SOURCE ==\"\n",
    ">&2 echo \"  [*] Downloading $FASTA_SOURCE reference genome...\"\n",
    "wget $FASTA_URL -O \"$FASTA_SOURCE\".fa.gz -q --continue\n",
    ">&2 echo \"  [*] Downloading SpikeIn reference FASTA...\"\n",
    "wget $SPIKEIN_FASTA -O \"$FASTA_SOURCE\".SpikeIn.fa.gz -q --continue\n",
    ">&2 echo \"  [*] Downloading PhiX reference FASTA...\"\n",
    "wget $PHIX_FASTA -O \"$FASTA_SOURCE\".PhiX.fa.gz -q --continue\n",
    "\n",
    ">&2 echo \"  [*] Unzipping fastas...\"\n",
    "gunzip -c \"$FASTA_SOURCE\".fa.gz \"$FASTA_SOURCE\".SpikeIn.fa.gz \"$FASTA_SOURCE\".PhiX.fa.gz> \"$FASTA_SOURCE\".fa\n",
    "\n",
    ">&2 echo \"  [*] Creating the $FASTA_SOURCE sequence dictionary...\"\n",
    "picard CreateSequenceDictionary R=\"$FASTA_SOURCE\".fa O=\"$FASTA_SOURCE\".fa.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genomic Data Commons\n",
    "\n",
    "The GDC similarly makes all of their reference files available on one page [here](https://gdc.cancer.gov/about-data/data-harmonization-and-generation/gdc-reference-files). The docs currently specify that their genome is made up of the three following reference sets:\n",
    "\n",
    "* The standard `GRCh38_no_alt` FASTA.\n",
    "* A set of standard sequence decoys.\n",
    "* A set of viral sequences.\n",
    "\n",
    "To be safe, we will download their full, concatenated FASTA they provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRCh38.d1.vd1.fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== GDC ==\n",
      "  [*] Downloading GDC reference genome...\n",
      "  [*] Unzipping GDC reference genome...\n",
      "  [*] Creating the GDC sequence dictionary...\n",
      "INFO\t2019-07-07 00:06:05\tCreateSequenceDictionary\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    CreateSequenceDictionary -R GDC.fa -O GDC.fa.dict\n",
      "**********\n",
      "\n",
      "\n",
      "00:06:06.183 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/claymcleod/conda/envs/star-mapping/share/picard-2.20.2-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Sun Jul 07 00:06:06 CDT 2019] CreateSequenceDictionary OUTPUT=GDC.fa.dict REFERENCE=GDC.fa    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Sun Jul 07 00:06:06 CDT 2019] Executing as claymcleod@desktop-ubuntu on Linux 4.18.0-25-generic amd64; OpenJDK 64-Bit Server VM 11.0.1+13-LTS; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.2-SNAPSHOT\n",
      "[Sun Jul 07 00:06:22 CDT 2019] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.27 minutes.\n",
      "Runtime.totalMemory()=1048576000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "FASTA_SOURCE=\"GDC\"\n",
    "FASTA_URL=\"https://api.gdc.cancer.gov/data/254f697d-310d-4d7d-a27b-27fbf767a834\"\n",
    "\n",
    ">&2 echo \"== $FASTA_SOURCE ==\"\n",
    ">&2 echo \"  [*] Downloading $FASTA_SOURCE reference genome...\"\n",
    "wget $FASTA_URL -O \"$FASTA_SOURCE\".fa.tar.gz -q --continue\n",
    "\n",
    ">&2 echo \"  [*] Unzipping $FASTA_SOURCE reference genome...\"\n",
    "tar xfvz \"$FASTA_SOURCE\".fa.tar.gz\n",
    "mv GRCh38.d1.vd1.fa \"$FASTA_SOURCE\".fa\n",
    "\n",
    ">&2 echo \"  [*] Creating the $FASTA_SOURCE sequence dictionary...\"\n",
    "picard CreateSequenceDictionary R=\"$FASTA_SOURCE\".fa O=\"$FASTA_SOURCE\".fa.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPMed + GTEx RNA-seq pipeline\n",
    "\n",
    "The GTEx consortium and TOPMed program both use the [GTEx RNA-seq pipeline](https://github.com/broadinstitute/gtex-pipeline/tree/master/rnaseq) developed by the Broad Institute. This workflow processes a high number of samples and has high reputation, so it's worth taking a look at.\n",
    "\n",
    "Following the \"reference genome and annotation\" [section](https://github.com/broadinstitute/gtex-pipeline/tree/master/rnaseq#reference-genome-and-annotation) of their `README.md`, you are directed to the [TOPMed RNA-seq pipeline harmonization](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md) page. Reading the \"Reference files\" section of that documentation essentially lays out that they use the Broad Insitute's version of `GRCh38` and add the `ERCC SpikeIn` sequences. They provide both [a link to the Broad's original FASTA](https://software.broadinstitute.org/gatk/download/bundle) and [a link to their built FASTA](https://personal.broadinstitute.org/francois/topmed/Homo_sapiens_assembly38_noALT_noHLA_noDecoy_ERCC.tar.gz) (although, given it points to a personal page, I'm not sure how long this link will be valid. For now, we will use the personal link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homo_sapiens_assembly38_noALT_noHLA_noDecoy_ERCC.fasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== TOPMed ==\n",
      "  [*] Downloading TOPMed reference genome...\n",
      "  [*] Unzipping TOPMed reference genome...\n",
      "  [*] Creating the TOPMed sequence dictionary...\n",
      "INFO\t2019-07-07 00:19:05\tCreateSequenceDictionary\t\n",
      "\n",
      "********** NOTE: Picard's command line syntax is changing.\n",
      "**********\n",
      "********** For more information, please see:\n",
      "********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition)\n",
      "**********\n",
      "********** The command line looks like this in the new syntax:\n",
      "**********\n",
      "**********    CreateSequenceDictionary -R TOPMed.fa -O TOPMed.fa.dict\n",
      "**********\n",
      "\n",
      "\n",
      "00:19:05.708 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/claymcleod/conda/envs/star-mapping/share/picard-2.20.2-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Sun Jul 07 00:19:05 CDT 2019] CreateSequenceDictionary OUTPUT=TOPMed.fa.dict REFERENCE=TOPMed.fa    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n",
      "[Sun Jul 07 00:19:05 CDT 2019] Executing as claymcleod@desktop-ubuntu on Linux 4.18.0-25-generic amd64; OpenJDK 64-Bit Server VM 11.0.1+13-LTS; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.2-SNAPSHOT\n",
      "[Sun Jul 07 00:19:23 CDT 2019] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.29 minutes.\n",
      "Runtime.totalMemory()=1048576000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "FASTA_SOURCE=\"TOPMed\"\n",
    "FASTA_URL=\"https://personal.broadinstitute.org/francois/topmed/Homo_sapiens_assembly38_noALT_noHLA_noDecoy_ERCC.tar.gz\"\n",
    "\n",
    ">&2 echo \"== $FASTA_SOURCE ==\"\n",
    ">&2 echo \"  [*] Downloading $FASTA_SOURCE reference genome...\"\n",
    "wget $FASTA_URL -O \"$FASTA_SOURCE\".fa.tar.gz -q --continue\n",
    "\n",
    ">&2 echo \"  [*] Unzipping $FASTA_SOURCE reference genome...\"\n",
    "tar xfvz \"$FASTA_SOURCE\".fa.tar.gz Homo_sapiens_assembly38_noALT_noHLA_noDecoy_ERCC.fasta\n",
    "mv Homo_sapiens_assembly38_noALT_noHLA_noDecoy_ERCC.fasta \"$FASTA_SOURCE\".fa\n",
    "\n",
    ">&2 echo \"  [*] Creating the $FASTA_SOURCE sequence dictionary...\"\n",
    "picard CreateSequenceDictionary R=\"$FASTA_SOURCE\".fa O=\"$FASTA_SOURCE\".fa.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis\n",
    "\n",
    "Now that we have downloaded all of the FASTAs we want to explore and created a sequence dictionary for each, we can run through them with some quick Python code to generate a Markdown table laying out how the MD5s of each sequence compare.\n",
    "\n",
    "First, some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def parse_sq_line(sq_line):\n",
    "    sq_split_by_tab = sq_line.split(\"\\t\")      \n",
    "    # Dynamically look for sequence name and md5sum\n",
    "    sn = None\n",
    "    md5 = None\n",
    "    for col in sq_split_by_tab:\n",
    "        if col.startswith(\"SN:\"):\n",
    "            sn = col.replace(\"SN:\", \"\")\n",
    "        elif col.startswith(\"M5:\"):\n",
    "            md5 = col.replace(\"M5:\", \"\")\n",
    "                \n",
    "        if sn and md5:\n",
    "            break\n",
    "                    \n",
    "    if not sn or not md5:\n",
    "        print(f\"Could not parse SN and MD5 for SQ line: {sq}!\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    return sn, md5\n",
    "\n",
    "def blacklisted_sq(sq_name):\n",
    "    \"\"\"\n",
    "    Returns True if we do not want to compare this SQ tag.\n",
    "    \n",
    "    I have chosen a blacklist approach here to ensure any new sequences added to any\n",
    "    of the genomes must be manually triaged (and this notebook must be updated).\n",
    "    \"\"\"\n",
    "    \n",
    "    blacklisted = ['phiX', 'ERCC', 'chrUn_', '_random', 'CMV', 'HBV', 'HCV', 'HIV', 'KSHV', 'HTLV', 'SV40', 'HPV', 'MCV']\n",
    "    for b in blacklisted:\n",
    "        if b in sq_name:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def read_sequence_dict(file, blacklist=True):\n",
    "    results = {}\n",
    "    \n",
    "    with open(file) as f:\n",
    "        sqs = [line for line in f.readlines() if line.startswith(\"@SQ\")]\n",
    "        for sq in sqs:\n",
    "            sn, md5 = parse_sq_line(sq)\n",
    "            if blacklist and blacklisted_sq(sq):\n",
    "                continue\n",
    "            results[sn] = md5\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the main analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Sequence Name | NCBI (baseline) | ENCODE | GDC | TOPMed | Concordant |\n",
       "| - | - | - | - | - | - |\n",
       "| chr1 | `6aef897c3d6ff0c78aff06ac189178dd` | `6aef897c3d6ff0c78aff06ac189178dd` | `6aef897c3d6ff0c78aff06ac189178dd` | `6aef897c3d6ff0c78aff06ac189178dd` | True |\n",
       "| chr2 | `f98db672eb0993dcfdabafe2a882905c` | `f98db672eb0993dcfdabafe2a882905c` | `f98db672eb0993dcfdabafe2a882905c` | `f98db672eb0993dcfdabafe2a882905c` | True |\n",
       "| chr3 | `76635a41ea913a405ded820447d067b0` | `76635a41ea913a405ded820447d067b0` | `76635a41ea913a405ded820447d067b0` | `76635a41ea913a405ded820447d067b0` | True |\n",
       "| chr4 | `3210fecf1eb92d5489da4346b3fddc6e` | `3210fecf1eb92d5489da4346b3fddc6e` | `3210fecf1eb92d5489da4346b3fddc6e` | `3210fecf1eb92d5489da4346b3fddc6e` | True |\n",
       "| chr5 | `a811b3dc9fe66af729dc0dddf7fa4f13` | `a811b3dc9fe66af729dc0dddf7fa4f13` | `a811b3dc9fe66af729dc0dddf7fa4f13` | `a811b3dc9fe66af729dc0dddf7fa4f13` | True |\n",
       "| chr6 | `5691468a67c7e7a7b5f2a3a683792c29` | `5691468a67c7e7a7b5f2a3a683792c29` | `5691468a67c7e7a7b5f2a3a683792c29` | `5691468a67c7e7a7b5f2a3a683792c29` | True |\n",
       "| chr7 | `cc044cc2256a1141212660fb07b6171e` | `cc044cc2256a1141212660fb07b6171e` | `cc044cc2256a1141212660fb07b6171e` | `cc044cc2256a1141212660fb07b6171e` | True |\n",
       "| chr8 | `c67955b5f7815a9a1edfaa15893d3616` | `c67955b5f7815a9a1edfaa15893d3616` | `c67955b5f7815a9a1edfaa15893d3616` | `c67955b5f7815a9a1edfaa15893d3616` | True |\n",
       "| chr9 | `6c198acf68b5af7b9d676dfdd531b5de` | `6c198acf68b5af7b9d676dfdd531b5de` | `6c198acf68b5af7b9d676dfdd531b5de` | `6c198acf68b5af7b9d676dfdd531b5de` | True |\n",
       "| chr10 | `c0eeee7acfdaf31b770a509bdaa6e51a` | `c0eeee7acfdaf31b770a509bdaa6e51a` | `c0eeee7acfdaf31b770a509bdaa6e51a` | `c0eeee7acfdaf31b770a509bdaa6e51a` | True |\n",
       "| chr11 | `1511375dc2dd1b633af8cf439ae90cec` | `1511375dc2dd1b633af8cf439ae90cec` | `1511375dc2dd1b633af8cf439ae90cec` | `1511375dc2dd1b633af8cf439ae90cec` | True |\n",
       "| chr12 | `96e414eace405d8c27a6d35ba19df56f` | `96e414eace405d8c27a6d35ba19df56f` | `96e414eace405d8c27a6d35ba19df56f` | `96e414eace405d8c27a6d35ba19df56f` | True |\n",
       "| chr13 | `a5437debe2ef9c9ef8f3ea2874ae1d82` | `a5437debe2ef9c9ef8f3ea2874ae1d82` | `a5437debe2ef9c9ef8f3ea2874ae1d82` | `a5437debe2ef9c9ef8f3ea2874ae1d82` | True |\n",
       "| chr14 | `e0f0eecc3bcab6178c62b6211565c807` | `e0f0eecc3bcab6178c62b6211565c807` | `e0f0eecc3bcab6178c62b6211565c807` | `e0f0eecc3bcab6178c62b6211565c807` | True |\n",
       "| chr15 | `f036bd11158407596ca6bf3581454706` | `f036bd11158407596ca6bf3581454706` | `f036bd11158407596ca6bf3581454706` | `f036bd11158407596ca6bf3581454706` | True |\n",
       "| chr16 | `db2d37c8b7d019caaf2dd64ba3a6f33a` | `db2d37c8b7d019caaf2dd64ba3a6f33a` | `db2d37c8b7d019caaf2dd64ba3a6f33a` | `db2d37c8b7d019caaf2dd64ba3a6f33a` | True |\n",
       "| chr17 | `f9a0fb01553adb183568e3eb9d8626db` | `f9a0fb01553adb183568e3eb9d8626db` | `f9a0fb01553adb183568e3eb9d8626db` | `f9a0fb01553adb183568e3eb9d8626db` | True |\n",
       "| chr18 | `11eeaa801f6b0e2e36a1138616b8ee9a` | `11eeaa801f6b0e2e36a1138616b8ee9a` | `11eeaa801f6b0e2e36a1138616b8ee9a` | `11eeaa801f6b0e2e36a1138616b8ee9a` | True |\n",
       "| chr19 | `85f9f4fc152c58cb7913c06d6b98573a` | `85f9f4fc152c58cb7913c06d6b98573a` | `85f9f4fc152c58cb7913c06d6b98573a` | `85f9f4fc152c58cb7913c06d6b98573a` | True |\n",
       "| chr20 | `b18e6c531b0bd70e949a7fc20859cb01` | `b18e6c531b0bd70e949a7fc20859cb01` | `b18e6c531b0bd70e949a7fc20859cb01` | `b18e6c531b0bd70e949a7fc20859cb01` | True |\n",
       "| chr21 | `974dc7aec0b755b19f031418fdedf293` | `974dc7aec0b755b19f031418fdedf293` | `974dc7aec0b755b19f031418fdedf293` | `974dc7aec0b755b19f031418fdedf293` | True |\n",
       "| chr22 | `ac37ec46683600f808cdd41eac1d55cd` | `ac37ec46683600f808cdd41eac1d55cd` | `ac37ec46683600f808cdd41eac1d55cd` | `ac37ec46683600f808cdd41eac1d55cd` | True |\n",
       "| chrX | `2b3a55ff7f58eb308420c8a9b11cac50` | `2b3a55ff7f58eb308420c8a9b11cac50` | `2b3a55ff7f58eb308420c8a9b11cac50` | `2b3a55ff7f58eb308420c8a9b11cac50` | True |\n",
       "| chrY | `ce3e31103314a704255f3cd90369ecce` | `ce3e31103314a704255f3cd90369ecce` | `ce3e31103314a704255f3cd90369ecce` | `ce3e31103314a704255f3cd90369ecce` | True |\n",
       "| chrM | `c68f52674c9fb33aef52dcf399755519` | `c68f52674c9fb33aef52dcf399755519` | `c68f52674c9fb33aef52dcf399755519` | `c68f52674c9fb33aef52dcf399755519` | True |\n",
       "| chrEBV | `6743bd63b3ff2b5b8985d8933c53290a` | `6743bd63b3ff2b5b8985d8933c53290a` | `6743bd63b3ff2b5b8985d8933c53290a` | `6743bd63b3ff2b5b8985d8933c53290a` | True |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from glob import glob\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "# which reference dataset do you want to use as a baseline?\n",
    "baseline='NCBI'\n",
    "# default behavior is True which means to just compare the autosomes/sex chromosomes/common decoy seqs\n",
    "# if set to True, this script will complain about any mismatches for MD5 sums.\n",
    "# if set to False, the script knows there will be differences for any uncommmon sequences and just\n",
    "# prints out the ASCII table.\n",
    "blacklist_uncommon_seqs=True\n",
    "\n",
    "# Read all sequence dictionaries\n",
    "results = {}\n",
    "for file_name in glob(\"*.dict\"):\n",
    "    source_name = file_name.replace(\".fa.dict\", \"\")\n",
    "    results[source_name] = read_sequence_dict(file_name, blacklist=blacklist_uncommon_seqs)\n",
    "sources = sorted(results.keys())\n",
    "if baseline not in sources:\n",
    "    print(f\"Missing baseline source sequence dictionary: {baseline}!\")\n",
    "    sys.exit(1)\n",
    "sources.remove(baseline)\n",
    "sources = [baseline] + sources\n",
    "    \n",
    "# Detect all chrs in each source\n",
    "detectedChrs = set()\n",
    "for source in sources:\n",
    "    for sq in results[source].keys():\n",
    "        detectedChrs.add(sq)\n",
    "        \n",
    "# Accumulate chrs in a nice order for viewing. Append remaining to end in sorted order.\n",
    "allChrs = []\n",
    "for i in list(range(1, 23)) + ['X', 'Y', 'M', 'EBV']:\n",
    "    identifier = f'chr{i}'\n",
    "    if identifier in detectedChrs:\n",
    "        allChrs.append(identifier)\n",
    "        detectedChrs.remove(identifier)\n",
    "allChrs = allChrs + sorted(detectedChrs)\n",
    "\n",
    "# Print markdown table header\n",
    "lines = []\n",
    "lines.append(' | '.join(['Sequence Name'] + [sources[0] + \" (baseline)\"] + sources[1:] + ['Concordant']))\n",
    "lines.append(' | '.join(['-'] * (len(sources) + 2)))\n",
    "\n",
    "for _chr in allChrs:\n",
    "    line = [_chr]\n",
    "    concordant = True\n",
    "    concordant_md5 = None\n",
    "    for source in sources:\n",
    "        md5 = \"Not present\"\n",
    "        if _chr in results[source]:\n",
    "            md5 = results[source][_chr]\n",
    "        if not concordant_md5:\n",
    "            concordant_md5 = md5\n",
    "        else:\n",
    "            if concordant and concordant_md5 != md5:\n",
    "                if blacklist_uncommon_seqs:\n",
    "                    print(f\"{_chr} does not match for all sources!\")\n",
    "                concordant = False\n",
    "        line.append(\"`\" + md5 + \"`\")  \n",
    "    line.append('True' if concordant else 'False')\n",
    "    lines.append(' | '.join(line))  \n",
    "\n",
    "lines = ['| ' + line + ' |' for line in lines]\n",
    "md = '\\n'.join(lines)\n",
    "\n",
    "# You can embed this in your Jupyter notebook or print it to be included \n",
    "# elsewhere by commenting/uncommenting the following lines.\n",
    "\n",
    "display(Markdown(md))\n",
    "# print(md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
